{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06368d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Top2Vec module.\n",
    "\n",
    "    Source code adapted from https://github.com/ddangelov/Top2Vec and https://github.com/MaartenGr/BERTopic\n",
    "\"\"\"\n",
    "import re\n",
    "\n",
    "import umap\n",
    "import hdbscan\n",
    "\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from typing import Union, List, Tuple\n",
    "\n",
    "from sklearn.cluster import dbscan\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b523207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare constants\n",
    "NAME = \"top2vec\"\n",
    "\n",
    "# Set seed for reproducibility purposes\n",
    "SEED = 0\n",
    "\n",
    "# Initialize Stemmer\n",
    "STEMMER = PorterStemmer()\n",
    "\n",
    "# Get stopwords and remove punctutaions from them\n",
    "STOP_WORDS = [re.sub(r\"[^a-z]\", \"\", stopword) for stopword in stopwords.words(\"english\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84dd68f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logger\n",
    "logger = logging.getLogger(NAME)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "sh = logging.StreamHandler()\n",
    "sh.setFormatter(logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"))\n",
    "logger.addHandler(sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76a4df37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentence(sentence):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    sentence = sentence.lower()\n",
    "    \n",
    "    # Remove non-alphabetic characters\n",
    "    sentence = re.sub(r\"[^a-z ]\", \"\", sentence)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    tokens = [token for token in sentence.split() if token not in STOP_WORDS]\n",
    "    \n",
    "    # Perform stemming\n",
    "    tokens = [STEMMER.stem(word) for word in tokens]\n",
    "    \n",
    "    # Construct bigrams\n",
    "    bigrams = [\"_\".join(tokens[i:i+2]) for i in range(len(tokens)-1)]\n",
    "    \n",
    "    # Return tokens\n",
    "    return tokens + bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "94464ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Top2Vec:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 embedding_model: str = \"all-MiniLM-L6-v2\",\n",
    "                 umap_model: umap.UMAP = None,\n",
    "                 hdbscan_model: hdbscan.HDBSCAN = None,\n",
    "                 vectorizer_model: TfidfVectorizer = None,\n",
    "                 seed: int = SEED,\n",
    "                 logger: logging.Logger = logger,\n",
    "                ):\n",
    "        \n",
    "        # Validate logger\n",
    "        if not isinstance(logger, logging.Logger):\n",
    "            raise TypeError(\"logger needs to be an instance of a logging.Logger object.\")\n",
    "        \n",
    "        # Load embedding model\n",
    "        logger.info(f\"Loading {embedding_model} model.\") \n",
    "        \n",
    "        try:\n",
    "            self.embedding_model = SentenceTransformer(embedding_model)\n",
    "        \n",
    "        except:\n",
    "            raise ValueError(\"Please select a valid SentenceTransformers model.\")\n",
    "            \n",
    "        logger.info(f\"Loaded {embedding_model} model successfully.\")\n",
    "            \n",
    "        self.seed = seed\n",
    "        \n",
    "        # UMAP\n",
    "        self.umap_model = umap_model or umap.UMAP(n_neighbors = 15,\n",
    "                                                  n_components = 5,\n",
    "                                                  metric = \"cosine\",\n",
    "                                                  random_state = self.seed)\n",
    "        \n",
    "        # Set seed for HDBSCAN\n",
    "        np.random.seed(self.seed)\n",
    "        \n",
    "        # HDBSCAN\n",
    "        self.hdbscan_model = hdbscan_model or hdbscan.HDBSCAN(min_cluster_size = 15, # To experiment with other values\n",
    "                                                              metric = \"euclidean\",\n",
    "                                                              cluster_selection_method = \"eom\")\n",
    "        \n",
    "        # Vectorizer\n",
    "        self.vectorizer_model = vectorizer_model or TfidfVectorizer(analyzer = process_sentence)\n",
    "        self.vectorizer_model.build_analyzer()\n",
    "        \n",
    "        \n",
    "    def fit(self, documents: Union[List[str], pd.Series]):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        # Validate documents\n",
    "        if not (isinstance(documents, list) or isinstance(documents, pd.Series)):\n",
    "            raise TypeError(\"documents need to a list or pandas series of strings.\")\n",
    "            \n",
    "        if not all(isinstance(document, str) for document in documents):\n",
    "            raise TypeError(\"documents need to a list or pandas series of strings.\")\n",
    "        \n",
    "        # Obtain document embeddings\n",
    "        logger.info(\"Obtaining document embeddings.\")\n",
    "        self.document_embeddings = self.embedding_model.encode(documents,\n",
    "                                                               convert_to_numpy = True,\n",
    "                                                               normalize_embeddings = True)\n",
    "        \n",
    "        \n",
    "        # Obtain umap embeddings\n",
    "        logger.info(\"Creating lower dimension document embeddings.\")\n",
    "        umap_embeddings = self.umap_model.fit(self.document_embeddings).embedding_\n",
    "        \n",
    "        # Obtain hdbscan clusters\n",
    "        logger.info(\"Finding dense areas of documents.\")\n",
    "        clusters = self.hdbscan_model.fit(umap_embeddings)\n",
    "        \n",
    "        # Create topic vectors\n",
    "        logger.info(\"Finding topics.\")\n",
    "        self.create_topic_vectors(clusters.labels_)\n",
    "        \n",
    "        # Deduplicate topics\n",
    "        self.deduplicate_topics()\n",
    "        \n",
    "        # Assign topic to documents\n",
    "        self.doc_top, self.doc_dist = self.calculate_documents_topic()\n",
    "        \n",
    "        # Calculate topic_sizes\n",
    "        self.topic_sizes = self.calculate_topic_sizes()\n",
    "        \n",
    "        # Re-order topics\n",
    "        self.reorder_topics()\n",
    "        \n",
    "    \n",
    "    def create_topic_vectors(self, cluster_labels: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "            Method to calculate the topic vectors based on the arithmetic mean of all the \n",
    "            document embeddings in the same dense cluster.\n",
    "\n",
    "            Args\n",
    "            ----------\n",
    "            cluster_labels: np.ndarray\n",
    "                    cluster assigned to each document based on HDBSCAN algorithm.\n",
    "\n",
    "            Returns\n",
    "            ----------\n",
    "            None\n",
    "        \"\"\"\n",
    "        unique_labels = set(cluster_labels)\n",
    "        if -1 in unique_labels:\n",
    "              unique_labels.remove(-1)\n",
    "\n",
    "        self.topic_vectors = self.l2_normalize(\n",
    "            np.vstack([self.document_embeddings[np.where(cluster_labels == label)[0]]\n",
    "                       .mean(axis = 0) for label in unique_labels]))\n",
    "            \n",
    "            \n",
    "    def deduplicate_topics(self) -> None:\n",
    "        \"\"\"\n",
    "            Method to merge duplicate topics.\n",
    "\n",
    "            Returns\n",
    "            ----------\n",
    "            None\n",
    "        \"\"\"\n",
    "        _, labels = dbscan(X = self.topic_vectors,\n",
    "                           eps = 0.1,\n",
    "                           min_samples = 2,\n",
    "                           metric = \"cosine\")\n",
    "\n",
    "        duplicate_clusters = set(labels)\n",
    "\n",
    "        if len(duplicate_clusters) > 1 or -1 not in duplicate_clusters:\n",
    "            \n",
    "            # Unique topics\n",
    "            unique_topics = self.topic_vectors[np.where(labels == -1)[0]]\n",
    "\n",
    "            if -1 in duplicate_clusters:\n",
    "                duplicate_clusters.remove(-1)\n",
    "                \n",
    "            # Merge duplicate topics\n",
    "            for unique_label in duplicate_clusters:\n",
    "                unique_topics = np.vstack(\n",
    "                    [unique_topics, self.l2_normalize(self.topic_vectors[np.where(labels == unique_label)[0]]\n",
    "                                                      .mean(axis = 0))])\n",
    "            self.topic_vectors = unique_topics\n",
    "            \n",
    "            \n",
    "    def calculate_documents_topic(self, batch_size: int = 64) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "            Method to compute the topic and score of each document.\n",
    "\n",
    "            Args\n",
    "            ----------\n",
    "            batch_size: int (Optional, default 64)\n",
    "                    number of documents passed to the model per iteration.\n",
    "\n",
    "            Returns\n",
    "            ----------\n",
    "            (document_topics, document_scores): tuple of a pair of np.ndarray\n",
    "                    the topic assigned to and score of each document. \n",
    "        \"\"\"\n",
    "        doc_top, doc_dist = [], []\n",
    "        for start_index in range(0, len(self.document_embeddings), batch_size):\n",
    "            res = np.inner(self.document_embeddings[start_index: start_index + batch_size], \n",
    "                           self.topic_vectors)\n",
    "            doc_top.extend(np.argmax(res, axis = 1))\n",
    "            doc_dist.extend(np.max(res, axis = 1))\n",
    "    \n",
    "        return np.array(doc_top), np.array(doc_dist)\n",
    "    \n",
    "    \n",
    "    def calculate_topic_sizes(self) -> pd.Series:\n",
    "        \"\"\"\n",
    "            Method to calculate the topic sizes.\n",
    "\n",
    "            Returns\n",
    "            ----------\n",
    "            topic_sizes: pd.Series\n",
    "                    number of documents belonging to each topic.\n",
    "        \"\"\"\n",
    "        return pd.Series(self.doc_top).value_counts()\n",
    "\n",
    "\n",
    "    def reorder_topics(self) -> None:\n",
    "        \"\"\"\n",
    "            Method to sort the topics in descending order based on topic size.\n",
    "\n",
    "            Returns\n",
    "            ----------\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.topic_vectors = self.topic_vectors[self.topic_sizes.index]\n",
    "        old2new = dict(zip(self.topic_sizes.index, range(self.topic_sizes.index.shape[0])))\n",
    "        self.doc_top = np.array([old2new[i] for i in self.doc_top])\n",
    "        self.topic_sizes.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def l2_normalize(vectors: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "            Method to scale input vectors individually to unit l2 norm (vector length).\n",
    "\n",
    "            Args\n",
    "            ----------\n",
    "            vectors: np.ndarray\n",
    "                    the data to normalize.\n",
    "\n",
    "            Returns\n",
    "            ----------\n",
    "            normalized vectors: np.ndarray\n",
    "                    normalized input vectors.\n",
    "        \"\"\"\n",
    "        if vectors.ndim == 2:\n",
    "            return normalize(vectors)\n",
    "        return normalize(vectors.reshape(1, -1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c428e1d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6c5110f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>altid</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sa1a70ab8ef5</td>\n",
       "      <td>Davenport hits out at Wimbledon</td>\n",
       "      <td>World number one Lindsay Davenport has critic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ta497aea0e36</td>\n",
       "      <td>Camera phones are 'must-haves'</td>\n",
       "      <td>Four times more mobiles with cameras in them ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ta0f0fa26a93</td>\n",
       "      <td>US top of supercomputing charts</td>\n",
       "      <td>The US has pushed Japan off the top of the su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ba23aaa4f4bb</td>\n",
       "      <td>Trial begins of Spain's top banker</td>\n",
       "      <td>The trial of Emilio Botin, the chairman of Sp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>baa126aeb946</td>\n",
       "      <td>Safety alert as GM recalls cars</td>\n",
       "      <td>The world's biggest carmaker General Motors (...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          altid                               title  \\\n",
       "0  sa1a70ab8ef5     Davenport hits out at Wimbledon   \n",
       "1  ta497aea0e36      Camera phones are 'must-haves'   \n",
       "2  ta0f0fa26a93     US top of supercomputing charts   \n",
       "3  ba23aaa4f4bb  Trial begins of Spain's top banker   \n",
       "4  baa126aeb946     Safety alert as GM recalls cars   \n",
       "\n",
       "                                             content  \n",
       "0   World number one Lindsay Davenport has critic...  \n",
       "1   Four times more mobiles with cameras in them ...  \n",
       "2   The US has pushed Japan off the top of the su...  \n",
       "3   The trial of Emilio Botin, the chairman of Sp...  \n",
       "4   The world's biggest carmaker General Motors (...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_data = pd.read_csv(\"news_data.csv\")\n",
    "news_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a66c550a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split content by sentences\n",
    "from nltk.tokenize import sent_tokenize\n",
    "news_data[\"sentences\"] = news_data.content.apply(sent_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c760c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store each sentence in its own row\n",
    "news_data_sentence = news_data[[\"title\", \"sentences\"]].explode(column = \"sentences\", ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "38d1bc4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-04 22:15:57,953 - top2vec - INFO - Loading all-MiniLM-L6-v2 model.\n",
      "2021-12-04 22:16:17,741 - top2vec - INFO - Loaded all-MiniLM-L6-v2 model successfully.\n"
     ]
    }
   ],
   "source": [
    "topic_model = Top2Vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1bd6c3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-04 22:16:19,549 - top2vec - INFO - Obtaining document embeddings.\n",
      "2021-12-04 22:16:37,632 - top2vec - INFO - Creating lower dimension document embeddings.\n",
      "2021-12-04 22:16:45,265 - top2vec - INFO - Finding dense areas of documents.\n",
      "2021-12-04 22:16:45,320 - top2vec - INFO - Finding topics.\n"
     ]
    }
   ],
   "source": [
    "topic_model.fit(news_data_sentence.sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e99363ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 4, 4, 0])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.doc_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f36d3e60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.34310704, 0.3246033 , 0.50510144, ..., 0.40411115, 0.5399056 ,\n",
       "       0.3701176 ], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.doc_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1447d197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(topic_model.doc_top)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
