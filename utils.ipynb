{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58c00a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Top2Vec module.\n",
    "\n",
    "    Source code adapted from https://github.com/ddangelov/Top2Vec and https://github.com/MaartenGr/BERTopic\n",
    "\"\"\"\n",
    "import re\n",
    "\n",
    "import umap\n",
    "import hdbscan\n",
    "\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from typing import Union, List, Tuple\n",
    "\n",
    "from sklearn.cluster import dbscan\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f6887f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare constants\n",
    "NAME = \"top2vec\"\n",
    "\n",
    "# Set seed for reproducibility purposes\n",
    "SEED = 0\n",
    "\n",
    "# Initialize Stemmer\n",
    "STEMMER = PorterStemmer()\n",
    "\n",
    "# Get stopwords and remove punctutaions from them\n",
    "STOP_WORDS = [re.sub(r\"[^a-z]\", \"\", stopword) for stopword in stopwords.words(\"english\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a100f20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logger\n",
    "logger = logging.getLogger(NAME)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "sh = logging.StreamHandler()\n",
    "sh.setFormatter(logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"))\n",
    "logger.addHandler(sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59ee4ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentence(sentence):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    sentence = sentence.lower()\n",
    "    \n",
    "    # Remove non-alphabetic characters\n",
    "    sentence = re.sub(r\"[^a-z ]\", \"\", sentence)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    tokens = [token for token in sentence.split() if token not in STOP_WORDS]\n",
    "    \n",
    "    # Perform stemming\n",
    "    tokens = [STEMMER.stem(word) for word in tokens]\n",
    "    \n",
    "    # Construct bigrams\n",
    "    bigrams = [\"_\".join(tokens[i:i+2]) for i in range(len(tokens)-1)]\n",
    "    \n",
    "    # Return tokens\n",
    "    return tokens + bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "cdf829ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Top2Vec:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 embedding_model: str = \"all-MiniLM-L6-v2\",\n",
    "                 umap_model: umap.UMAP = None,\n",
    "                 hdbscan_model: hdbscan.HDBSCAN = None,\n",
    "                 vectorizer_model: TfidfVectorizer = None,\n",
    "                 seed: int = SEED,\n",
    "                 logger: logging.Logger = logger,\n",
    "                ):\n",
    "        \n",
    "        # Validate logger\n",
    "        if not isinstance(logger, logging.Logger):\n",
    "            raise TypeError(\"logger needs to be an instance of a logging.Logger object.\")\n",
    "        \n",
    "        # Load embedding model\n",
    "        logger.info(f\"Loading {embedding_model} model.\") \n",
    "        \n",
    "        try:\n",
    "            self.embedding_model = SentenceTransformer(embedding_model)\n",
    "        \n",
    "        except:\n",
    "            raise ValueError(\"Please select a valid SentenceTransformers model.\")\n",
    "            \n",
    "        logger.info(f\"Loaded {embedding_model} model successfully.\")\n",
    "            \n",
    "        self.seed = seed\n",
    "        self.results = None\n",
    "        \n",
    "        # UMAP\n",
    "        self.umap_model = umap_model or umap.UMAP(n_neighbors = 15,\n",
    "                                                  n_components = 5,\n",
    "                                                  metric = \"cosine\",\n",
    "                                                  random_state = self.seed)\n",
    "        \n",
    "        # Set seed for HDBSCAN\n",
    "        np.random.seed(self.seed)\n",
    "        \n",
    "        # HDBSCAN\n",
    "        self.hdbscan_model = hdbscan_model or hdbscan.HDBSCAN(min_cluster_size = 15, # To experiment with other values\n",
    "                                                              metric = \"euclidean\",\n",
    "                                                              cluster_selection_method = \"eom\")\n",
    "        \n",
    "        # Vectorizer\n",
    "        self.vectorizer_model = vectorizer_model or TfidfVectorizer(analyzer = process_sentence)\n",
    "        self.vectorizer_model.build_analyzer()\n",
    "        \n",
    "        \n",
    "    def fit(self, documents: Union[List[str], pd.Series]):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        # Validate documents\n",
    "        if not (isinstance(documents, list) or isinstance(documents, pd.Series)):\n",
    "            raise TypeError(\"documents need to a list or pandas series of strings.\")\n",
    "            \n",
    "        if not all(isinstance(document, str) for document in documents):\n",
    "            raise TypeError(\"documents need to a list or pandas series of strings.\")\n",
    "        \n",
    "        columns = [\"document\"]\n",
    "        if isinstance(documents, list):\n",
    "            self.results = pd.DataFrame(documents, columns = columns)\n",
    "        \n",
    "        if isinstance(documents, pd.Series):\n",
    "            self.results = documents.to_frame(name = columns[0])\n",
    "        \n",
    "        # Obtain document embeddings\n",
    "        logger.info(\"Obtaining document embeddings.\")\n",
    "        self.document_embeddings = self.embedding_model.encode(documents,\n",
    "                                                               convert_to_numpy = True,\n",
    "                                                               normalize_embeddings = True)\n",
    "        \n",
    "        # Obtain umap embeddings\n",
    "        logger.info(\"Creating lower dimension document embeddings.\")\n",
    "        umap_embeddings = self.umap_model.fit(self.document_embeddings).embedding_\n",
    "        \n",
    "        # Obtain hdbscan clusters\n",
    "        logger.info(\"Finding dense areas of documents.\")\n",
    "        clusters = self.hdbscan_model.fit(umap_embeddings)\n",
    "        \n",
    "        # Create topic vectors\n",
    "        logger.info(\"Finding topics.\")\n",
    "        self.create_topic_vectors(clusters.labels_)\n",
    "        \n",
    "        # Deduplicate topics\n",
    "        self.deduplicate_topics()\n",
    "        \n",
    "        # Assign topic to documents\n",
    "        self.doc_top, self.doc_dist = self.calculate_documents_topic()\n",
    "        \n",
    "        # Calculate topic_sizes\n",
    "        self.topic_sizes = self.calculate_topic_sizes()\n",
    "        \n",
    "        # Re-order topics\n",
    "        self.reorder_topics()\n",
    "        \n",
    "        # Append clustering results to dataframe\n",
    "        self.results[\"topic\"], self.results[\"score\"] = self.doc_top, self.doc_dist\n",
    "        \n",
    "        # Sort results by topic and score\n",
    "        self.results.sort_values(\n",
    "            by = [\"topic\", \"score\"], ascending = [True, False], inplace = True)\n",
    "        \n",
    "        self.results.reset_index(drop = True, inplace = True)\n",
    "        \n",
    "    \n",
    "    def create_topic_vectors(self, cluster_labels: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "            Method to calculate the topic vectors based on the arithmetic mean of all the \n",
    "            document embeddings in the same dense cluster.\n",
    "\n",
    "            Args\n",
    "            ----------\n",
    "            cluster_labels: np.ndarray\n",
    "                    cluster assigned to each document based on HDBSCAN algorithm.\n",
    "\n",
    "            Returns\n",
    "            ----------\n",
    "            None\n",
    "        \"\"\"\n",
    "        unique_labels = set(cluster_labels)\n",
    "        if -1 in unique_labels:\n",
    "              unique_labels.remove(-1)\n",
    "\n",
    "        self.topic_vectors = self.l2_normalize(\n",
    "            np.vstack([self.document_embeddings[np.where(cluster_labels == label)[0]]\n",
    "                       .mean(axis = 0) for label in unique_labels]))\n",
    "            \n",
    "            \n",
    "    def deduplicate_topics(self) -> None:\n",
    "        \"\"\"\n",
    "            Method to merge duplicate topics.\n",
    "\n",
    "            Returns\n",
    "            ----------\n",
    "            None\n",
    "        \"\"\"\n",
    "        _, labels = dbscan(X = self.topic_vectors,\n",
    "                           eps = 0.1,\n",
    "                           min_samples = 2,\n",
    "                           metric = \"cosine\")\n",
    "\n",
    "        duplicate_clusters = set(labels)\n",
    "\n",
    "        if len(duplicate_clusters) > 1 or -1 not in duplicate_clusters:\n",
    "            \n",
    "            # Unique topics\n",
    "            unique_topics = self.topic_vectors[np.where(labels == -1)[0]]\n",
    "\n",
    "            if -1 in duplicate_clusters:\n",
    "                duplicate_clusters.remove(-1)\n",
    "                \n",
    "            # Merge duplicate topics\n",
    "            for unique_label in duplicate_clusters:\n",
    "                unique_topics = np.vstack(\n",
    "                    [unique_topics, self.l2_normalize(self.topic_vectors[np.where(labels == unique_label)[0]]\n",
    "                                                      .mean(axis = 0))])\n",
    "            self.topic_vectors = unique_topics\n",
    "            \n",
    "            \n",
    "    def calculate_documents_topic(self, batch_size: int = 64) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "            Method to compute the topic and score of each document.\n",
    "\n",
    "            Args\n",
    "            ----------\n",
    "            batch_size: int (Optional, default 64)\n",
    "                    number of documents passed to the model per iteration.\n",
    "\n",
    "            Returns\n",
    "            ----------\n",
    "            (document_topics, document_scores): tuple of a pair of np.ndarray\n",
    "                    the topic assigned to and score of each document. \n",
    "        \"\"\"\n",
    "        doc_top, doc_dist = [], []\n",
    "        for start_index in range(0, len(self.document_embeddings), batch_size):\n",
    "            res = np.inner(self.document_embeddings[start_index: start_index + batch_size], \n",
    "                           self.topic_vectors)\n",
    "            doc_top.extend(np.argmax(res, axis = 1))\n",
    "            doc_dist.extend(np.max(res, axis = 1))\n",
    "    \n",
    "        return np.array(doc_top), np.array(doc_dist)\n",
    "    \n",
    "    \n",
    "    def calculate_topic_sizes(self) -> pd.Series:\n",
    "        \"\"\"\n",
    "            Method to calculate the topic sizes.\n",
    "\n",
    "            Returns\n",
    "            ----------\n",
    "            topic_sizes: pd.Series\n",
    "                    number of documents belonging to each topic.\n",
    "        \"\"\"\n",
    "        return pd.Series(self.doc_top).value_counts() \\\n",
    "                                      .to_frame(name = \"count\") \\\n",
    "                                      .reset_index() \\\n",
    "                                      .rename(columns = {\"index\": \"topic\"})\n",
    "\n",
    "\n",
    "    def reorder_topics(self) -> None:\n",
    "        \"\"\"\n",
    "            Method to sort the topics in descending order based on topic size.\n",
    "\n",
    "            Returns\n",
    "            ----------\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.topic_vectors = self.topic_vectors[self.topic_sizes.index]\n",
    "        old2new = dict(zip(self.topic_sizes.index, range(self.topic_sizes.index.shape[0])))\n",
    "        self.doc_top = np.array([old2new[i] for i in self.doc_top])\n",
    "        self.topic_sizes.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        \n",
    "    def get_results(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        return self.results\n",
    "        \n",
    "        \n",
    "    def get_summary(self, top_n_documents: int = 10) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        return self.results.groupby(\"topic\").head(top_n_documents).reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    def get_top_n_terms(self, top_n_terms: int = 15):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        # Compute document term matrix\n",
    "        document_term_matrix = self.vectorizer_model.fit_transform(\n",
    "            self.results.groupby(\"topic\", as_index = False).agg({\"document\": \" \".join}).document\n",
    "        ).toarray()\n",
    "\n",
    "        # Get vocabulary\n",
    "        vocab = self.vectorizer_model.get_feature_names()\n",
    "\n",
    "        # Generate the top n words per topic\n",
    "        return pd.DataFrame(\n",
    "            [(doc, vocab[word], document_term_matrix[doc][word]) \n",
    "            for doc in docs_per_topic[\"topic\"] \n",
    "            for word in document_term_matrix.argsort(axis=1)[:, -top_n_terms:][doc][::-1]],\n",
    "            columns = [\"topic\", \"term\", \"score\"])\n",
    "            \n",
    "        \n",
    "    @staticmethod\n",
    "    def l2_normalize(vectors: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "            Method to scale input vectors individually to unit l2 norm (vector length).\n",
    "\n",
    "            Args\n",
    "            ----------\n",
    "            vectors: np.ndarray\n",
    "                    the data to normalize.\n",
    "\n",
    "            Returns\n",
    "            ----------\n",
    "            normalized vectors: np.ndarray\n",
    "                    normalized input vectors.\n",
    "        \"\"\"\n",
    "        if vectors.ndim == 2:\n",
    "            return normalize(vectors)\n",
    "        return normalize(vectors.reshape(1, -1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b2f738",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c2f4a33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>altid</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sa1a70ab8ef5</td>\n",
       "      <td>Davenport hits out at Wimbledon</td>\n",
       "      <td>World number one Lindsay Davenport has critic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ta497aea0e36</td>\n",
       "      <td>Camera phones are 'must-haves'</td>\n",
       "      <td>Four times more mobiles with cameras in them ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ta0f0fa26a93</td>\n",
       "      <td>US top of supercomputing charts</td>\n",
       "      <td>The US has pushed Japan off the top of the su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ba23aaa4f4bb</td>\n",
       "      <td>Trial begins of Spain's top banker</td>\n",
       "      <td>The trial of Emilio Botin, the chairman of Sp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>baa126aeb946</td>\n",
       "      <td>Safety alert as GM recalls cars</td>\n",
       "      <td>The world's biggest carmaker General Motors (...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          altid                               title  \\\n",
       "0  sa1a70ab8ef5     Davenport hits out at Wimbledon   \n",
       "1  ta497aea0e36      Camera phones are 'must-haves'   \n",
       "2  ta0f0fa26a93     US top of supercomputing charts   \n",
       "3  ba23aaa4f4bb  Trial begins of Spain's top banker   \n",
       "4  baa126aeb946     Safety alert as GM recalls cars   \n",
       "\n",
       "                                             content  \n",
       "0   World number one Lindsay Davenport has critic...  \n",
       "1   Four times more mobiles with cameras in them ...  \n",
       "2   The US has pushed Japan off the top of the su...  \n",
       "3   The trial of Emilio Botin, the chairman of Sp...  \n",
       "4   The world's biggest carmaker General Motors (...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_data = pd.read_csv(\"news_data.csv\")\n",
    "news_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "578ac75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split content by sentences\n",
    "from nltk.tokenize import sent_tokenize\n",
    "news_data[\"sentences\"] = news_data.content.apply(sent_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "925654f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store each sentence in its own row\n",
    "news_data_sentence = news_data[[\"title\", \"sentences\"]].explode(column = \"sentences\", ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7676cda3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fa8ecb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_per_topic = topic_model.results.groupby(\"topic\", as_index = False).agg({\"document\": \" \".join})\n",
    "topic_col = \"topic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f48b108",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d5fe902a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-04 23:49:35,640 - top2vec - INFO - Loading all-MiniLM-L6-v2 model.\n",
      "2021-12-04 23:49:52,234 - top2vec - INFO - Loaded all-MiniLM-L6-v2 model successfully.\n"
     ]
    }
   ],
   "source": [
    "topic_model = Top2Vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "dee8ea57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-04 23:49:52,252 - top2vec - INFO - Obtaining document embeddings.\n",
      "2021-12-04 23:50:00,718 - top2vec - INFO - Creating lower dimension document embeddings.\n",
      "2021-12-04 23:50:03,923 - top2vec - INFO - Finding dense areas of documents.\n",
      "2021-12-04 23:50:03,945 - top2vec - INFO - Finding topics.\n"
     ]
    }
   ],
   "source": [
    "topic_model.fit(news_data_sentence.sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9d0c1800",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\addison\\anaconda3\\envs\\ocbc\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>term</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>lift</td>\n",
       "      <td>0.204609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>speed</td>\n",
       "      <td>0.186008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>xbox</td>\n",
       "      <td>0.167408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>tower</td>\n",
       "      <td>0.150987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>allard</td>\n",
       "      <td>0.129418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>12</td>\n",
       "      <td>search_email</td>\n",
       "      <td>0.077325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>12</td>\n",
       "      <td>googl</td>\n",
       "      <td>0.077325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>12</td>\n",
       "      <td>user_search</td>\n",
       "      <td>0.077325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>12</td>\n",
       "      <td>web_search</td>\n",
       "      <td>0.077325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>12</td>\n",
       "      <td>outlook</td>\n",
       "      <td>0.066682</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>195 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     topic          term     score\n",
       "0        0          lift  0.204609\n",
       "1        0         speed  0.186008\n",
       "2        0          xbox  0.167408\n",
       "3        0         tower  0.150987\n",
       "4        0        allard  0.129418\n",
       "..     ...           ...       ...\n",
       "190     12  search_email  0.077325\n",
       "191     12         googl  0.077325\n",
       "192     12   user_search  0.077325\n",
       "193     12    web_search  0.077325\n",
       "194     12       outlook  0.066682\n",
       "\n",
       "[195 rows x 3 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_top_n_terms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c5e8e85a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>topic</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Xbox frontman J. Allard said the console looke...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.686321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Some details of the Xbox's performance and wha...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.612559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Blue Gene/L machine that will be completed...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.595026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The previous top machine, Japan's NEC Earth Si...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.593119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Since the first supercomputer, the Cray-1, was...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.591385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1134</th>\n",
       "      <td>It is designed to work alongside Microsoft's O...</td>\n",
       "      <td>12</td>\n",
       "      <td>0.421039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1135</th>\n",
       "      <td>These have included giving people easier ways ...</td>\n",
       "      <td>12</td>\n",
       "      <td>0.396545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1136</th>\n",
       "      <td>The select committee found that those responsi...</td>\n",
       "      <td>12</td>\n",
       "      <td>0.325824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1137</th>\n",
       "      <td>Around 10,000 people visited the campaign's we...</td>\n",
       "      <td>12</td>\n",
       "      <td>0.216941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1138</th>\n",
       "      <td>A new attraction - Walt Disney Studios - has r...</td>\n",
       "      <td>12</td>\n",
       "      <td>0.176798</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1139 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               document  topic     score\n",
       "0     Xbox frontman J. Allard said the console looke...      0  0.686321\n",
       "1     Some details of the Xbox's performance and wha...      0  0.612559\n",
       "2     The Blue Gene/L machine that will be completed...      0  0.595026\n",
       "3     The previous top machine, Japan's NEC Earth Si...      0  0.593119\n",
       "4     Since the first supercomputer, the Cray-1, was...      0  0.591385\n",
       "...                                                 ...    ...       ...\n",
       "1134  It is designed to work alongside Microsoft's O...     12  0.421039\n",
       "1135  These have included giving people easier ways ...     12  0.396545\n",
       "1136  The select committee found that those responsi...     12  0.325824\n",
       "1137  Around 10,000 people visited the campaign's we...     12  0.216941\n",
       "1138  A new attraction - Walt Disney Studios - has r...     12  0.176798\n",
       "\n",
       "[1139 rows x 3 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "90aa3108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>topic</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Xbox frontman J. Allard said the console looke...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.686321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Some details of the Xbox's performance and wha...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.612559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Blue Gene/L machine that will be completed...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.595026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The previous top machine, Japan's NEC Earth Si...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.593119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Since the first supercomputer, the Cray-1, was...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.591385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>The firm is following in the footsteps of Micr...</td>\n",
       "      <td>12</td>\n",
       "      <td>0.699218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>The desktop search technology has been license...</td>\n",
       "      <td>12</td>\n",
       "      <td>0.690468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>\"Desktop search is just one of many features p...</td>\n",
       "      <td>12</td>\n",
       "      <td>0.682206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>\"We are all getting more and more files on our...</td>\n",
       "      <td>12</td>\n",
       "      <td>0.638044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>Search engines are often the first port of cal...</td>\n",
       "      <td>12</td>\n",
       "      <td>0.631191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>130 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              document  topic     score\n",
       "0    Xbox frontman J. Allard said the console looke...      0  0.686321\n",
       "1    Some details of the Xbox's performance and wha...      0  0.612559\n",
       "2    The Blue Gene/L machine that will be completed...      0  0.595026\n",
       "3    The previous top machine, Japan's NEC Earth Si...      0  0.593119\n",
       "4    Since the first supercomputer, the Cray-1, was...      0  0.591385\n",
       "..                                                 ...    ...       ...\n",
       "125  The firm is following in the footsteps of Micr...     12  0.699218\n",
       "126  The desktop search technology has been license...     12  0.690468\n",
       "127  \"Desktop search is just one of many features p...     12  0.682206\n",
       "128  \"We are all getting more and more files on our...     12  0.638044\n",
       "129  Search engines are often the first port of cal...     12  0.631191\n",
       "\n",
       "[130 rows x 3 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6dd517",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
